{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1015ebe",
   "metadata": {},
   "source": [
    "# Pandas設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bd2b0",
   "metadata": {},
   "source": [
    "# 包含Category的欄位大綱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欄位大綱\n",
    "df.describe(include=object).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c036581",
   "metadata": {},
   "source": [
    "# 數數各欄位有幾個值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17501dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欄位值 unique 數量\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a77c4e",
   "metadata": {},
   "source": [
    "# 把所有欄位的histogram秀出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85738da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,hue='gender',diag_kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a74835",
   "metadata": {},
   "source": [
    "# 做numeric column的Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TSNE to do cluster with numeric column\n",
    "TSNE(learning_rate=50) # learning rate 通常介於 10 ~ 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b18c2",
   "metadata": {},
   "source": [
    "# 把Variance過小的欄位移除(因為資料幾乎都長一樣，沒有區別)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "sel.fit(head_df / head_df.mean()) # normalization\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced dataframe\n",
    "reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "\n",
    "# VarianceThreshold可以Filter掉 variance小於Threshold的columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d89e50",
   "metadata": {},
   "source": [
    "# 建一個只有一半的corr heatmap(看起來更清楚)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df02426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建一個只有一半的corr heatmap(看起來更清楚)\n",
    "\n",
    "corr = df.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "sns.heatmap(\n",
    "    df.corr(),\n",
    "    mask=mask,\n",
    "    center=0,\n",
    "    cmap=cmap,\n",
    "    linewidths=1,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79bb47",
   "metadata": {},
   "source": [
    "# 移除corr過高(太相似)的columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a4ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除corr過高(太相似)的columns\n",
    "\n",
    "# Calculate the correlation matrix and take the absolute value\n",
    "corr_matrix = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "tri_df = corr_matrix.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "reduced_df = ansur_df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcbaf4c",
   "metadata": {},
   "source": [
    "# 把做完logistic regression後的coef結果 做成dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c782bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把做完logistic regression後的coef結果 做成dict\n",
    "lr.coef_  # coef array\n",
    "\n",
    "dict(zip(X.columns,lr.coef_[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55df92f",
   "metadata": {},
   "source": [
    "# RFE，基本上跟上一CELL做一樣的事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc77396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE基本上跟上一CELL做一樣的事\n",
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1, step=1)\n",
    "\n",
    "# Fits the eliminator to the data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_))) # ranking是各column排名，名次越高越晚被刪掉\n",
    "\n",
    "# Print the features that are not eliminated\n",
    "print(X.columns[rfe.support_]) # 列出沒被刪掉的column\n",
    "\n",
    "# Calculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a399188",
   "metadata": {},
   "source": [
    "# 用Lasso 把coef =0 的欄位抓出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df653ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用Lasso 把coef =0 的欄位抓出來\n",
    "\n",
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452ac82",
   "metadata": {},
   "source": [
    "# 用Lasso 把coef 小於0.01的移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用Lasso 把coef 小於0.01的移除\n",
    "\n",
    "# Find the highest alpha value with R-squared above 98%\n",
    "la = Lasso(alpha=0.01, random_state=0)\n",
    "\n",
    "# Fits the model and calculates performance stats\n",
    "la.fit(X_train_std, y_train)\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "# Print peformance stats \n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346fff3",
   "metadata": {},
   "source": [
    "# Using LassoCV找最棒的alpha, 並找出coef不為0的欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LassoCV找最棒的alpha, 並找出coef不為0的欄位\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train,y_train)\n",
    "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test,y_test)\n",
    "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_!=0\n",
    "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab56837c",
   "metadata": {},
   "source": [
    "# 用REF&GradientBoostingRegressor找最棒的column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用REF&GradientBoostingRegressor找最棒的column\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "gb_mask = rfe_gb.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d97d1",
   "metadata": {},
   "source": [
    "# 用REF&RandomForestRegressor找最棒的column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用REF&RandomForestRegressor找最棒的column\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "rf_mask = rfe_rf.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96558928",
   "metadata": {},
   "source": [
    "# 把剛剛三個結果組合起來，找出三個method都通過的column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002981c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把剛剛三個結果組合起來，找出三個method都通過的column\n",
    "\n",
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes >= 3\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "X_reduced = X.loc[:, meta_mask]\n",
    "\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003ae27",
   "metadata": {},
   "source": [
    "# 計算欄位平均值並assign to新欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean height\n",
    "height_df['height'] = height_df[['height_1', 'height_2'  ,'height_3']].mean(axis=1)\n",
    "\n",
    "# Drop the 3 original height features\n",
    "reduced_df = height_df.drop(['height_1', 'height_2'  ,'height_3'], axis=1)\n",
    "\n",
    "print(reduced_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54581ac8",
   "metadata": {},
   "source": [
    "# 顯示componet分布圖 (用PCA前記得要先scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = PCA()\n",
    "pc = pca.fit_transform(ansur_std)\n",
    "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "\n",
    "# Create a pairplot of the principal component dataframe\n",
    "sns.pairplot(pc_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161c5d6",
   "metadata": {},
   "source": [
    "# 將各component的累積比重顯示出來 用cumsum方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b74201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the cumulative sum of the explained variance ratio\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c851e",
   "metadata": {},
   "source": [
    "# 將各component的Vector參數顯示出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"reducer\", PCA(n_components=2))])\n",
    "\n",
    "# Fit it to the dataset and extract the component vectors\n",
    "pipe.fit(poke_df)\n",
    "vectors = pipe.steps[1][1].components_.round(2)\n",
    "\n",
    "# Print feature effects\n",
    "print(\"PC 1 effects = \" + str(dict(zip(poke_df.columns, vectors[0]))))\n",
    "print(\"PC 2 effects = \" + str(dict(zip(poke_df.columns, vectors[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee06ea",
   "metadata": {},
   "source": [
    "# 顯示各component的scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ff9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "poke_cat_df['PC 1'] = pc[:, 0]\n",
    "poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "# Use the Legendary feature to color the PC 1 vs PC 2 scatterplot\n",
    "sns.scatterplot(data=poke_cat_df, \n",
    "                x='PC 1', y='PC 2', hue='Legendary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43854f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reducer', PCA(n_components=3)),\n",
    "        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the explained variance ratio and accuracy\n",
    "print(pipe.steps[1][1].explained_variance_ratio_)\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe0c61",
   "metadata": {},
   "source": [
    "# pca.fit_transform反過來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pca.fit_transform(X)\n",
    "\n",
    "X = pca.inverse_transform(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98a9b4",
   "metadata": {},
   "source": [
    "# 設定PCA要占多少比重 using n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA(n_components=0.8) # 要保留80%比重\n",
    "\n",
    "PCA(n_components=5) # 要生五個Components出來"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b818f7c",
   "metadata": {},
   "source": [
    "# 用if 調整欄位內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Aspect\"][train_df[\"Aspect\"] < 0] += 360\n",
    "train_df[\"Aspect\"][train_df[\"Aspect\"] > 359] -= 360\n",
    "\n",
    "train_df.loc[train_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n",
    "test_df.loc[test_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c254d1",
   "metadata": {},
   "source": [
    "# 更改datatype來降低記憶體使用，太神啦77777777777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05052829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    " \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab871ef6",
   "metadata": {},
   "source": [
    "# Using normalize and dot to get similarities of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a80ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the NMF features: norm_features\n",
    "norm_features = normalize(nmf_features)\n",
    "\n",
    "# Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features,index=titles)\n",
    "\n",
    "# Select the row corresponding to 'Cristiano Ronaldo': article\n",
    "article = df.loc['Cristiano Ronaldo']\n",
    "\n",
    "# Compute the dot products: similarities\n",
    "similarities = df.dot(article)\n",
    "\n",
    "# Display those with the largest cosine similarity\n",
    "print(similarities.nlargest())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bec700",
   "metadata": {},
   "source": [
    "# Using Lasso and C to find best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify L1 regularization\n",
    "lr = LogisticRegression(penalty='l1') # l2 = Ridge, l1 = Lasso\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Find the number of nonzero coefficients (selected features)\n",
    "best_lr = searcher.best_estimator_\n",
    "coefs = best_lr.coef_\n",
    "print(\"Total number of features:\", coefs.size)\n",
    "print(\"Number of selected features:\", np.count_nonzero(coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6ae5b",
   "metadata": {},
   "source": [
    "# Reverse array order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inds_descending, inds_ascending are array\n",
    "inds_descending = inds_ascending[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c340e28d",
   "metadata": {},
   "source": [
    "# XGBClassifier sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e362d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066477e1",
   "metadata": {},
   "source": [
    "# Trees as base learners example: Scikit-learn API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f040f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(seed=123,objective=\"reg:linear\",n_estimators=10)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ef9ea",
   "metadata": {},
   "source": [
    "# Linear base learners example: learning API only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(X_train,y_train)\n",
    "DM_test =  xgb.DMatrix(X_test,y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(dtrain = DM_train, params=params, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7146f",
   "metadata": {},
   "source": [
    "# XGBoost find best param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "for reg in reg_params:\n",
    "\n",
    "    # Update l2 strength\n",
    "    params[\"lambda\"] = reg\n",
    "    \n",
    "    # Pass this updated param dictionary into cv\n",
    "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "print(\"Best rmse as a function of l2:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc169da1",
   "metadata": {},
   "source": [
    "# XGB feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(X,y)\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":4}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params,dtrain=housing_dmatrix,num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364fe11",
   "metadata": {},
   "source": [
    "# LabelEncoder example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fill missing values with 0\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "\n",
    "# Print the head of the categorical columns\n",
    "print(df[categorical_columns].head())\n",
    "\n",
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "# Print the head of the LabelEncoded categorical columns\n",
    "print(df[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5ebfb",
   "metadata": {},
   "source": [
    "# OneHotEncoder example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = OneHotEncoder(categorical_features=categorical_mask,sparse=False)\n",
    "\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "df_encoded = ohe.fit_transform(df)\n",
    "\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "print(df_encoded[:5, :])\n",
    "\n",
    "# Print the shape of the original DataFrame\n",
    "print(df.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48ed6a",
   "metadata": {},
   "source": [
    "# Using linkage and hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae01487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the fcluster and linkage functions\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage() function\n",
    "distance_matrix = linkage(comic_con[['x_scaled', 'y_scaled']], method = 'ward', metric = 'euclidean')\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'] = fcluster(distance_matrix, 2, criterion='maxclust')\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f99704",
   "metadata": {},
   "source": [
    "# KMeans and VQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(comic_con[['x_scaled','y_scaled']],2)\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'], distortion_list = vq(comic_con[['x_scaled','y_scaled']],cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e856e",
   "metadata": {},
   "source": [
    "# lineplot and Elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b86120",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "num_clusters = range(2, 7)\n",
    "\n",
    "# Create a list of distortions from the kmeans function\n",
    "for i in num_clusters:\n",
    "    cluster_centers, distortion = kmeans(uniform_data[['x_scaled','y_scaled']],i)\n",
    "    distortions.append(distortion)\n",
    "\n",
    "# Create a data frame with two lists - number of clusters and distortions\n",
    "elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})\n",
    "\n",
    "# Creat a line plot of num_clusters and distortions\n",
    "sns.lineplot(x='num_clusters', y='distortions', data=elbow_plot)\n",
    "plt.xticks(num_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d17dc",
   "metadata": {},
   "source": [
    "# KMeans and VQ example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ab4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create centroids with kmeans for 2 clusters\n",
    "cluster_centers,_ = kmeans(fifa[scaled_features], 2)\n",
    "\n",
    "# Assign cluster labels and print cluster centers\n",
    "fifa['cluster_labels'], _ = vq(fifa[scaled_features], cluster_centers)\n",
    "print(fifa.groupby('cluster_labels')[scaled_features].mean())\n",
    "\n",
    "# Plot cluster centers to visualize clusters\n",
    "fifa.groupby('cluster_labels')[scaled_features].mean().plot(legend=True, kind='bar')\n",
    "plt.show()\n",
    "\n",
    "# Get the name column of first 5 players in each cluster\n",
    "for cluster in fifa['cluster_labels'].unique():\n",
    "    print(cluster, fifa[fifa['cluster_labels'] == cluster]['name'].values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04369dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pandas.core.dtypes.missing.notna(obj)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.notna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dab72f",
   "metadata": {},
   "source": [
    "# Using regular expression to extract word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.match(pattern, length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking[\"Length\"].apply(lambda row: return_mileage(row))\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5fcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d259cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the rest of the parameters\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e49a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba409aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760305c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0accc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142fcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c9027e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd618d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbe2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
