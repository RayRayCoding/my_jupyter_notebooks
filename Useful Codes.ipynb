{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1015ebe",
   "metadata": {},
   "source": [
    "# Pandas設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bd2b0",
   "metadata": {},
   "source": [
    "# 包含Category的欄位大綱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欄位大綱\n",
    "df.describe(include=object).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c036581",
   "metadata": {},
   "source": [
    "# 數數各欄位有幾個值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17501dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欄位值 unique 數量\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a77c4e",
   "metadata": {},
   "source": [
    "# 把所有欄位的histogram秀出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85738da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,hue='gender',diag_kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a74835",
   "metadata": {},
   "source": [
    "# 做numeric column的Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TSNE to do cluster with numeric column\n",
    "TSNE(learning_rate=50) # learning rate 通常介於 10 ~ 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b18c2",
   "metadata": {},
   "source": [
    "# 把Variance過小的欄位移除(因為資料幾乎都長一樣，沒有區別)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "sel.fit(head_df / head_df.mean()) # normalization\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced dataframe\n",
    "reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "\n",
    "# VarianceThreshold可以Filter掉 variance小於Threshold的columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d89e50",
   "metadata": {},
   "source": [
    "# 建一個只有一半的corr heatmap(看起來更清楚)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df02426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建一個只有一半的corr heatmap(看起來更清楚)\n",
    "\n",
    "corr = df.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "sns.heatmap(\n",
    "    df.corr(),\n",
    "    mask=mask,\n",
    "    center=0,\n",
    "    cmap=cmap,\n",
    "    linewidths=1,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79bb47",
   "metadata": {},
   "source": [
    "# 移除corr過高(太相似)的columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a4ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除corr過高(太相似)的columns\n",
    "\n",
    "# Calculate the correlation matrix and take the absolute value\n",
    "corr_matrix = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "tri_df = corr_matrix.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "reduced_df = ansur_df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcbaf4c",
   "metadata": {},
   "source": [
    "# 把做完logistic regression後的coef結果 做成dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c782bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把做完logistic regression後的coef結果 做成dict\n",
    "lr.coef_  # coef array\n",
    "\n",
    "dict(zip(X.columns,lr.coef_[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55df92f",
   "metadata": {},
   "source": [
    "# RFE，基本上跟上一CELL做一樣的事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc77396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE基本上跟上一CELL做一樣的事\n",
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1, step=1)\n",
    "\n",
    "# Fits the eliminator to the data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_))) # ranking是各column排名，名次越高越晚被刪掉\n",
    "\n",
    "# Print the features that are not eliminated\n",
    "print(X.columns[rfe.support_]) # 列出沒被刪掉的column\n",
    "\n",
    "# Calculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a399188",
   "metadata": {},
   "source": [
    "# 用Lasso 把coef =0 的欄位抓出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df653ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用Lasso 把coef =0 的欄位抓出來\n",
    "\n",
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452ac82",
   "metadata": {},
   "source": [
    "# 用Lasso 把coef 小於0.01的移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用Lasso 把coef 小於0.01的移除\n",
    "\n",
    "# Find the highest alpha value with R-squared above 98%\n",
    "la = Lasso(alpha=0.01, random_state=0)\n",
    "\n",
    "# Fits the model and calculates performance stats\n",
    "la.fit(X_train_std, y_train)\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "# Print peformance stats \n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346fff3",
   "metadata": {},
   "source": [
    "# Using LassoCV找最棒的alpha, 並找出coef不為0的欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LassoCV找最棒的alpha, 並找出coef不為0的欄位\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train,y_train)\n",
    "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test,y_test)\n",
    "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_!=0\n",
    "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab56837c",
   "metadata": {},
   "source": [
    "# 用REF&GradientBoostingRegressor找最棒的column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用REF&GradientBoostingRegressor找最棒的column\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "gb_mask = rfe_gb.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d97d1",
   "metadata": {},
   "source": [
    "# 用REF&RandomForestRegressor找最棒的column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用REF&RandomForestRegressor找最棒的column\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "rf_mask = rfe_rf.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96558928",
   "metadata": {},
   "source": [
    "# 把剛剛三個結果組合起來，找出三個method都通過的column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002981c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把剛剛三個結果組合起來，找出三個method都通過的column\n",
    "\n",
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes >= 3\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "X_reduced = X.loc[:, meta_mask]\n",
    "\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003ae27",
   "metadata": {},
   "source": [
    "# 計算欄位平均值並assign to新欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean height\n",
    "height_df['height'] = height_df[['height_1', 'height_2'  ,'height_3']].mean(axis=1)\n",
    "\n",
    "# Drop the 3 original height features\n",
    "reduced_df = height_df.drop(['height_1', 'height_2'  ,'height_3'], axis=1)\n",
    "\n",
    "print(reduced_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54581ac8",
   "metadata": {},
   "source": [
    "# 顯示componet分布圖 (用PCA前記得要先scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = PCA()\n",
    "pc = pca.fit_transform(ansur_std)\n",
    "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "\n",
    "# Create a pairplot of the principal component dataframe\n",
    "sns.pairplot(pc_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161c5d6",
   "metadata": {},
   "source": [
    "# 將各component的累積比重顯示出來 用cumsum方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b74201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the cumulative sum of the explained variance ratio\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c851e",
   "metadata": {},
   "source": [
    "# 將各component的Vector參數顯示出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"reducer\", PCA(n_components=2))])\n",
    "\n",
    "# Fit it to the dataset and extract the component vectors\n",
    "pipe.fit(poke_df)\n",
    "vectors = pipe.steps[1][1].components_.round(2)\n",
    "\n",
    "# Print feature effects\n",
    "print(\"PC 1 effects = \" + str(dict(zip(poke_df.columns, vectors[0]))))\n",
    "print(\"PC 2 effects = \" + str(dict(zip(poke_df.columns, vectors[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee06ea",
   "metadata": {},
   "source": [
    "# 顯示各component的scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ff9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "poke_cat_df['PC 1'] = pc[:, 0]\n",
    "poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "# Use the Legendary feature to color the PC 1 vs PC 2 scatterplot\n",
    "sns.scatterplot(data=poke_cat_df, \n",
    "                x='PC 1', y='PC 2', hue='Legendary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43854f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reducer', PCA(n_components=3)),\n",
    "        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the explained variance ratio and accuracy\n",
    "print(pipe.steps[1][1].explained_variance_ratio_)\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe0c61",
   "metadata": {},
   "source": [
    "# pca.fit_transform反過來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pca.fit_transform(X)\n",
    "\n",
    "X = pca.inverse_transform(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98a9b4",
   "metadata": {},
   "source": [
    "# 設定PCA要占多少比重 using n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA(n_components=0.8) # 要保留80%比重\n",
    "\n",
    "PCA(n_components=5) # 要生五個Components出來"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b818f7c",
   "metadata": {},
   "source": [
    "# 用if 調整欄位內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Aspect\"][train_df[\"Aspect\"] < 0] += 360\n",
    "train_df[\"Aspect\"][train_df[\"Aspect\"] > 359] -= 360\n",
    "\n",
    "train_df.loc[train_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n",
    "test_df.loc[test_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c254d1",
   "metadata": {},
   "source": [
    "# 更改datatype來降低記憶體使用，太神啦77777777777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05052829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    " \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433feb7c",
   "metadata": {},
   "source": [
    "# 用KMeans做Elbow Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a80ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 6)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(samples)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef6e22a",
   "metadata": {},
   "source": [
    "# 將KMeans結果做成crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KMeans model with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Use fit_predict to fit model and obtain cluster labels: labels\n",
    "labels = model.fit_predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'],df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f15ec7",
   "metadata": {},
   "source": [
    "# One way to make pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17bdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler,kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23912b6",
   "metadata": {},
   "source": [
    "# Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(samples,method='complete')\n",
    "\n",
    "# Plot the dendrogram, using varieties as labels\n",
    "dendrogram(mergings,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ccdfa",
   "metadata": {},
   "source": [
    "# 用TSNE 做Cluster\n",
    "## TSNE每次的圖都會長不太一樣，但cluster相對關係不變\n",
    "## learning_rate 50~200，多嘗試幾個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "model = TSNE(learning_rate=200)\n",
    "\n",
    "# Apply fit_transform to samples: tsne_features\n",
    "tsne_features = model.fit_transform(samples)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1st feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot, coloring by variety_numbers\n",
    "plt.scatter(xs,ys,c=variety_numbers)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319be094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0573c624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfebf81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
